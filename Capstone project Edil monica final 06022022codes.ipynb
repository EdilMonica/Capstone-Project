{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Numpy Version',np.__version__)\n",
    "print('Pandas Version',pd.__version__)\n",
    "print('Seaborn Version',sns.__version__)\n",
    "print('Matplotlib Version',matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_palette(palette='deep')\n",
    "!pip install folium\n",
    "import folium\n",
    "from folium.plugins import FastMarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Data\n",
    "df = pd.read_excel(\"Customer Churn Data.xlsx\",sheet_name=\"Data for DSBA\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information \n",
    "Info=df.info()\n",
    "Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatypes=df.dtypes.value_counts()\n",
    "datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Target\n",
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicate\n",
    "dup_rows = df.duplicated()\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.shape\n",
    "print('Number of duplicate rows = %d' % (dup_rows.sum()))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value counts for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the gender variables\n",
    "df.Gender.replace(to_replace='F',value='Female',inplace=True)\n",
    "df.Gender.replace(to_replace='M',value='Male',inplace=True)\n",
    "df.Gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payments\n",
    "df.Payment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the account segment variables\n",
    "df.account_segment.replace(to_replace='Regular Plus',value='Regular +',inplace=True)\n",
    "df.account_segment.replace(to_replace='Super Plus',value='Super +',inplace=True)\n",
    "df.account_segment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital Status\n",
    "df.Marital_Status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login device\n",
    "df.Login_device.replace(to_replace='&&&&',value='NAN',inplace=True)\n",
    "df.Login_device.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding median value to replace with special characters\n",
    "df[df['Tenure']!= '#'].Tenure.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Account_user_count']!= '@'].Account_user_count.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['rev_per_month']!= '+'].rev_per_month.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['rev_growth_yoy']!= '$'].rev_growth_yoy.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cashback']!= '$'].cashback.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Day_Since_CC_connect']!= '$'].Day_Since_CC_connect.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coupon_used_for_payment.replace(to_replace='*',value='NAN',inplace=True)\n",
    "df.coupon_used_for_payment.replace(to_replace='$',value='NAN',inplace=True)\n",
    "df.coupon_used_for_payment.replace(to_replace='#',value='NAN',inplace=True)\n",
    "df[df['coupon_used_for_payment']!= '$'].coupon_used_for_payment.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the spl characters\n",
    "df.Tenure.replace(to_replace='#',value='9',inplace=True)\n",
    "df.Account_user_count.replace(to_replace='@',value='4',inplace=True)\n",
    "df.rev_per_month.replace(to_replace='+',value='5',inplace=True)\n",
    "df.rev_growth_yoy.replace(to_replace='$',value='15',inplace=True)\n",
    "df.Day_Since_CC_connect.replace(to_replace='$',value='3',inplace=True)\n",
    "df.cashback.replace(to_replace='$',value='165',inplace=True)\n",
    "df.coupon_used_for_payment.replace(to_replace='NAN',value='1',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tenure'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropped the unwanted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('AccountID',axis=1,inplace=True) # Removed the AccountID cplumn which is not important variable in the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of target variables after dropping AccountID\n",
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of Target variable\n",
    "df['Churn'].value_counts()*100/len(df),2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "null=df.isna().sum()\n",
    "null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values\n",
    "Null_Perc=round(df.isna().sum()*100/len(df),2)\n",
    "Null_Perc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since the missing values are not more than 4 percentage we do not need to drop the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing with median for numerical data\n",
    "median1=df[\"Tenure\"].median()\n",
    "median2=df[\"City_Tier\"].median()\n",
    "median3=df[\"CC_Contacted_LY\"].median()\n",
    "median4=df[\"Service_Score\"].median()\n",
    "median5=df[\"Account_user_count\"].median()\n",
    "median6=df[\"CC_Agent_Score\"].median()\n",
    "median7=df[\"rev_per_month\"].median()\n",
    "median8=df[\"Complain_ly\"].median()\n",
    "median9=df[\"Day_Since_CC_connect\"].median()\n",
    "median10=df[\"cashback\"].median()\n",
    "\n",
    "\n",
    "df[\"Tenure\"].replace(np.nan,median1,inplace=True)\n",
    "df[\"City_Tier\"].replace(np.nan,median2,inplace=True)\n",
    "df[\"CC_Contacted_LY\"].replace(np.nan,median3,inplace=True)\n",
    "df[\"Service_Score\"].replace(np.nan,median4,inplace=True)\n",
    "df[\"Account_user_count\"].replace(np.nan,median5,inplace=True)\n",
    "df[\"CC_Agent_Score\"].replace(np.nan,median6,inplace=True)\n",
    "df[\"rev_per_month\"].replace(np.nan,median7,inplace=True)\n",
    "df[\"Complain_ly\"].replace(np.nan,median8,inplace=True)\n",
    "df[\"Day_Since_CC_connect\"].replace(np.nan,median9,inplace=True)\n",
    "df[\"cashback\"].replace(np.nan,median10,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Tenure\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"City_Tier\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CC_Contacted_LY\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Service_Score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Account_user_count\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CC_Agent_Score\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rev_per_month\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Complain_ly\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Day_Since_CC_connect\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cashback\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing NULL values in Categorical Columns using Mode\n",
    "mode1=df[\"Payment\"].mode().values[0]\n",
    "mode2=df[\"Gender\"].mode().values[0]\n",
    "mode3=df[\"account_segment\"].mode().values[0]\n",
    "mode4=df[\"Marital_Status\"].mode().values[0]\n",
    "mode5=df[\"Login_device\"].mode().values[0]\n",
    "\n",
    "\n",
    "df[\"Payment\"]=df[\"Payment\"].replace(np.nan,mode1)\n",
    "df[\"Gender\"]= df[\"Gender\"].replace(np.nan,mode2)\n",
    "df[\"account_segment\"]=df[\"account_segment\"].replace(np.nan,mode3)\n",
    "df[\"Marital_Status\"]=df[\"Marital_Status\"].replace(np.nan,mode4)\n",
    "df[\"Login_device\"]=df[\"Login_device\"].replace(np.nan,mode5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null values after imputing\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting into int datatypes\n",
    "df[\"Tenure\"] = df.Tenure.astype(int)\n",
    "df[\"Account_user_count\"] = df.Account_user_count.astype(int)\n",
    "df[\"rev_per_month\"] = df.rev_per_month.astype(int)\n",
    "df[\"rev_growth_yoy\"] = df.rev_growth_yoy.astype(int)\n",
    "df[\"coupon_used_for_payment\"] = df.coupon_used_for_payment.astype(int)\n",
    "df[\"Day_Since_CC_connect\"] = df.Day_Since_CC_connect.astype(int)\n",
    "df[\"cashback\"] = df.cashback.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatype value after converting datatypes\n",
    "df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the data\n",
    "df.describe(include='all').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['Churn','Tenure','City_Tier','CC_Contacted_LY','Service_Score','Account_user_count','CC_Agent_Score','rev_per_month','Complain_ly','rev_growth_yoy','coupon_used_for_payment','Day_Since_CC_connect','cashback'];\n",
    "\n",
    "for i in cols:\n",
    "    sns.boxplot(df[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Numerical_count_plot = ['City_Tier','Service_Score','Complain_ly','CC_Agent_Score','coupon_used_for_payment','Account_user_count']\n",
    "fig,axes = plt.subplots(2,3,figsize=(20,10))\n",
    "plt.subplots_adjust(hspace=0.3,wspace=0.2)\n",
    "for i,j in zip(Numerical_count_plot,axes.flatten()):\n",
    "    sns.countplot(x=i,data = df, ax=j,order = list(df[i].value_counts().index))\n",
    "    j.set_title('Countplot: ' +i.upper());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis for categorical variable\n",
    "def univariateAnalysis_category(cat_column):\n",
    "    print(\"Details of \" + cat_column)\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(df_cat[cat_column].value_counts())\n",
    "    plt.figure()\n",
    "    df_cat[cat_column].value_counts().plot.bar(title=\"Frequency Distribution of \" + cat_column)\n",
    "    plt.show()\n",
    "    print(\"       \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.select_dtypes(include = ['object'])\n",
    "lstcatcolumns = list(df_cat.columns.values)\n",
    "lstcatcolumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in lstcatcolumns:\n",
    "    univariateAnalysis_category(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Bivariate analysis\n",
    "plt.figure(figsize=(20,15))\n",
    "cor_matrix = df.corr()\n",
    "sns.heatmap(cor_matrix, cmap = 'Blues', annot=True, fmt='.2f', vmin = -1, vmax= 1,mask=np.triu(cor_matrix,+1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the count of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ol = df.drop('Churn',axis=1)\n",
    "Q1 = df_ol.quantile(0.25)\n",
    "Q3 = df_ol.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "UL = Q3 + 1.5*IQR\n",
    "LL = Q1 - 1.5*IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df_ol > UL) | (df_ol < LL)).sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column wise outliers percentage\n",
    "((df_ol > UL) | (df_ol < LL)).sum().sort_values(ascending = False)/df_ol.index.size*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Converting all objects to categorical codes\n",
    "for feature in df.columns: \n",
    "    if df[feature].dtype == 'object': \n",
    "        print('\\n')\n",
    "        print('feature:',feature)\n",
    "        print(pd.Categorical(df[feature].unique()))\n",
    "        print(pd.Categorical(df[feature].unique()).codes)\n",
    "        df[feature] = pd.Categorical(df[feature]).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, diag_kind='kde', corner = True, hue = 'Churn')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.pie(df.Churn.value_counts(1)*100,labels=df.Churn.value_counts().index,autopct='%1.2f%%', radius=1,textprops={'fontsize': 20})\n",
    "plt.pie([1],colors=['w'],radius=0.33)\n",
    "plt.title('Donut Chart showing Churn distribution',size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "cplot = sns.countplot(df.Churn)\n",
    "plt.title('Bar Chart showing Churn distribution',size=20)\n",
    "for p in cplot.patches:\n",
    "        cplot.annotate(format(int(p.get_height())),(p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                   ha = 'center', va = 'center', size=20, color='w' ,xytext = (0, -25),textcoords = 'offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =df.drop('Churn', axis=1)\n",
    "y=df['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Churn'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_vif(X).sort_values(by='VIF', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we see that the value of VIF is high for many variables. Here, we may drop variables with VIF more than 10 (very high correlation) & build our model\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = 'Churn ~ Tenure + CC_Contacted_LY + Payment + Gender + account_segment + CC_Agent_Score + Marital_Status + rev_per_month + Complain_ly+ coupon_used_for_payment+ Day_Since_CC_connect + cashback + Login_device'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as SM\n",
    "model_1 = SM.logit(formula = f_1, data=df).fit()\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_2 = 'Churn ~ Tenure +  CC_Contacted_LY + Gender + account_segment + CC_Agent_Score + Marital_Status + rev_per_month + Complain_ly +coupon_used_for_payment+ Day_Since_CC_connect + Login_device'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = SM.logit(formula = f_2, data=df).fit()\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_summary = pd.DataFrame(model_2.params,columns=['coef'])\n",
    "impt = coef_summary.drop('Intercept').sort_values('coef',ascending = False)\n",
    "impt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=impt.coef,y=impt.index)\n",
    "plt.title('Feature Importance',size=15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing unnecessary features for model building to arrive the final dataset \n",
    "df.drop(['Account_user_count','rev_growth_yoy','Service_Score','Payment','cashback'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the target column into separate vectors for training set and test set\n",
    "X =df.drop('Churn', axis=1)\n",
    "y=df.pop('Churn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, train_labels, test_labels = train_test_split(X, y, test_size=.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the dimensions of the training and test data\n",
    "print('X_train',X_train.shape)\n",
    "print('X_test',X_test.shape)\n",
    "print('train_labels',train_labels.shape)\n",
    "print('test_labels',test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing liabaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,roc_curve,classification_report,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [5,10,15,20],\n",
    "    'min_samples_leaf': [10,20,30,40], \n",
    "    'min_samples_split': [30,40,50,100],\n",
    "}\n",
    "\n",
    "dtcl = DecisionTreeClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = dtcl, param_grid = param_grid, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, train_labels)\n",
    "print(grid_search.best_params_)\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_char_label = ['no', 'yes']\n",
    "tree_regularized = open('tree_regularized.dot','w')\n",
    "dot_data = tree.export_graphviz(best_grid, out_file= tree_regularized , feature_names = list(X_train), class_names = list(train_char_label))\n",
    "\n",
    "tree_regularized.close()\n",
    "dot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.webgraphviz.com/?tab=map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict = best_grid.predict(X_train)\n",
    "ytest_predict = best_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_predict\n",
    "ytest_predict_prob=best_grid.predict_proba(X_test)\n",
    "ytest_predict_prob\n",
    "pd.DataFrame(ytest_predict_prob).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for Training data\n",
    "cart_train_acc=best_grid.score(X_train,train_labels) \n",
    "cart_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "probs = best_grid.predict_proba(X_train)\n",
    "probs = probs[:, 1]\n",
    "cart_train_auc = roc_auc_score(train_labels, probs)\n",
    "cart_train_fpr, cart_train_tpr, cart_train_thresholds = roc_curve(train_labels, probs)\n",
    "print('AUC: %.3f' % cart_train_auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_train_fpr, cart_train_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, ytrain_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels,ytrain_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 values\n",
    "cart_metrics=classification_report(train_labels, ytrain_predict,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_train_f1=round(df.loc[\"1\"][2],2)\n",
    "cart_train_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_train_precision=round(df.loc[\"1\"][0],2)\n",
    "print ('cart_train_precision ',cart_train_precision)\n",
    "print ('cart_train_recall ',cart_train_recall)\n",
    "print ('cart_train_f1 ',cart_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cart_test_acc=best_grid.score(X_test,test_labels) \n",
    "cart_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = best_grid.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "cart_test_auc = roc_auc_score(test_labels, probs)\n",
    "cart_test_fpr, cart_test_tpr, cart_test_thresholds = roc_curve(test_labels, probs)\n",
    "print('AUC: %.3f' % cart_test_auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_test_fpr, cart_test_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, ytest_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels,ytest_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 values\n",
    "cart_metrics=classification_report(test_labels, ytest_predict,output_dict=True)\n",
    "df=pd.DataFrame(cart_metrics).transpose()\n",
    "cart_test_f1=round(df.loc[\"1\"][2],2)\n",
    "cart_test_recall=round(df.loc[\"1\"][1],2)\n",
    "cart_test_precision=round(df.loc[\"1\"][0],2)\n",
    "print ('cart_test_precision ',cart_test_precision)\n",
    "print ('cart_test_recall ',cart_test_recall)\n",
    "print ('cart_test_f1 ',cart_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [20,40,60],\n",
    "    'max_features': [10,20],\n",
    "    'min_samples_leaf': [5,10],\n",
    "    'min_samples_split': [50,100],\n",
    "    'n_estimators': [500,700,1000],\n",
    "    \n",
    "}\n",
    "rfcl=RandomForestClassifier()\n",
    "grid_search_RF=GridSearchCV(estimator=rfcl,param_grid=param_grid,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.fit(X_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_RF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_RF=grid_search_RF.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict_RF = best_grid_RF.predict(X_train)\n",
    "ytest_predict_RF = best_grid_RF.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_acc=best_grid_RF.score(X_train,train_labels)\n",
    "rf_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels,ytrain_predict_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_fpr, rf_train_tpr,_=roc_curve(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "rf_train_auc=roc_auc_score(train_labels,best_grid.predict_proba(X_train)[:,1])\n",
    "print('Area under Curve is', rf_train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels,ytrain_predict_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 values\n",
    "rf_metrics=classification_report(train_labels, ytrain_predict_RF,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_train_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_train_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_train_precision ',rf_train_precision)\n",
    "print ('rf_train_recall ',rf_train_recall)\n",
    "print ('rf_train_f1 ',rf_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_acc=best_grid_RF.score(X_test,test_labels)\n",
    "rf_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels,ytest_predict_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_test_fpr, rf_test_tpr,_=roc_curve(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "plt.plot(rf_test_fpr,rf_test_tpr,color='green')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "rf_test_auc=roc_auc_score(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "print('Area under Curve is', rf_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels,ytest_predict_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pd.DataFrame(best_grid_RF.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values('Imp',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 values\n",
    "rf_metrics=classification_report(test_labels, ytest_predict_RF,output_dict=True)\n",
    "df=pd.DataFrame(rf_metrics).transpose()\n",
    "rf_test_precision=round(df.loc[\"1\"][0],2)\n",
    "rf_test_recall=round(df.loc[\"1\"][1],2)\n",
    "rf_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('rf_test_precision ',rf_test_precision)\n",
    "print ('rf_test_recall ',rf_test_recall)\n",
    "print ('rf_test_f1 ',rf_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_trains = sc.fit_transform(X_train)\n",
    "X_tests = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'hidden_layer_sizes': [500,800,1000],\n",
    "    'max_iter': [500,1000,1500], \n",
    "    'solver': ['adam'],\n",
    "    'tol': [0.001], \n",
    "}\n",
    "\n",
    "nncl = MLPClassifier(random_state=1)\n",
    "\n",
    "grid_search_NN = GridSearchCV(estimator = nncl, param_grid = param_grid, cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_NN.fit(X_train, train_labels)\n",
    "grid_search_NN.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid_NN = grid_search_NN.best_estimator_\n",
    "best_grid_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_predict_NN = best_grid_NN.predict(X_train)\n",
    "ytest_predict_NN = best_grid_NN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Performance Evaluation on Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_acc=best_grid_NN.score(X_train,train_labels) \n",
    "nn_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels,ytrain_predict_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_train_fpr, nn_train_tpr,_=roc_curve(train_labels,best_grid_NN.predict_proba(X_train)[:,1])\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_train_auc=roc_auc_score(train_labels,best_grid_NN.predict_proba(X_train)[:,1])\n",
    "print('Area under Curve is', nn_train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels,ytrain_predict_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(train_labels, ytrain_predict_NN,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_train_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_train_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_train_precision ',nn_train_precision)\n",
    "print ('nn_train_recall ',nn_train_recall)\n",
    "print ('nn_train_f1 ',nn_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_acc=best_grid_NN.score(X_test,test_labels)\n",
    "nn_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels,ytest_predict_NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_fpr, nn_test_tpr,_=roc_curve(test_labels,best_grid_NN.predict_proba(X_test)[:,1])\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "nn_test_auc=roc_auc_score(test_labels,best_grid.predict_proba(X_test)[:,1])\n",
    "print('Area under Curve is', nn_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels,ytest_predict_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_metrics=classification_report(test_labels, ytest_predict_NN,output_dict=True)\n",
    "df=pd.DataFrame(nn_metrics).transpose()\n",
    "nn_test_precision=round(df.loc[\"1\"][0],2)\n",
    "nn_test_recall=round(df.loc[\"1\"][1],2)\n",
    "nn_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('nn_test_precision ',nn_test_precision)\n",
    "print ('nn_test_recall ',nn_test_recall)\n",
    "print ('nn_test_f1 ',nn_test_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='newton-cg',max_iter=30000,penalty='none',verbose=True,n_jobs=2)\n",
    "model.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain_predict = model.predict(X_train)\n",
    "Ytest_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest_predict_prob=model.predict_proba(X_test)\n",
    "pd.DataFrame(Ytest_predict_prob).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR model Performance Evaluation on Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train_acc=model.score(X_train,train_labels)\n",
    "lr_train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_train)\n",
    "probs = probs[:, 1]\n",
    "lr_train_auc = roc_auc_score(train_labels, probs)\n",
    "lr_train_fpr, lr_train_tpr, train_thresholds = roc_curve(train_labels, probs)\n",
    "print('AUC: %.3f' % lr_train_auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(lr_train_fpr, lr_train_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(lr_train_tpr - lr_train_fpr)\n",
    "optimal_threshold = train_thresholds[optimal_idx]\n",
    "optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred=[]\n",
    "for i in range(0,len(probs)):\n",
    "    if np.array(probs)[i]>0.25:\n",
    "        a=1\n",
    "    else:\n",
    "        a=0\n",
    "    y_train_pred.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(train_labels, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metrics=classification_report(train_labels, y_train_pred,output_dict=True)\n",
    "df=pd.DataFrame(lr_metrics).transpose()\n",
    "lr_train_precision=round(df.loc[\"1\"][0],2)\n",
    "lr_train_recall=round(df.loc[\"1\"][1],2)\n",
    "lr_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('lr_train_precision ',lr_train_precision)\n",
    "print ('lr_train_recall ',lr_train_recall)\n",
    "print ('lr_train_f1 ',lr_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR model Performance Evaluation on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_test_acc=model.score(X_test, test_labels)\n",
    "lr_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "probs = probs[:, 1]\n",
    "lr_test_auc = roc_auc_score(test_labels, probs)\n",
    "lr_test_fpr, lr_test_tpr, lr_test_thresholds = roc_curve(test_labels, probs)\n",
    "print('AUC: %.3f' % lr_test_auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(lr_test_fpr, lr_test_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred=[]\n",
    "for i in range(0,len(probs)):\n",
    "    if np.array(probs)[i]>0.25:\n",
    "        a=1\n",
    "    else:\n",
    "        a=0\n",
    "    y_test_pred.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_labels, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_metrics=classification_report(test_labels, y_test_pred,output_dict=True)\n",
    "df=pd.DataFrame(lr_metrics).transpose()\n",
    "lr_test_precision=round(df.loc[\"1\"][0],2)\n",
    "lr_test_recall=round(df.loc[\"1\"][1],2)\n",
    "lr_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('lr_train_precision ',lr_train_precision)\n",
    "print ('lr_train_recall ',lr_train_recall)\n",
    "print ('lr_train_f1 ',lr_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_model=KNeighborsClassifier()\n",
    "KNN_model.fit(X_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn = knn.fit(X_trains,train_labels)\n",
    "y_train_predict_knn = knn.predict(X_trains)\n",
    "y_test_predict_knn = knn.predict(X_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Matrix on train data set\n",
    "y_train_predict = KNN_model.predict(X_train)\n",
    "knn_model_score = KNN_model.score(X_train, train_labels)\n",
    "print(knn_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_knn = knn.score(X_trains,train_labels) \n",
    "acc_test_knn = knn.score(X_tests,test_labels) \n",
    "print('train accuracy :', round(acc_train_knn,4))\n",
    "print('test accuracy :', round(acc_test_knn,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Matrix on test data set\n",
    "y_test_predict = KNN_model.predict(X_test)\n",
    "model_score = KNN_model.score(X_test, test_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us find optimum value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_scores = []\n",
    "\n",
    "for k in range(1,20,2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_trains,train_labels)\n",
    "    scores = knn.score(X_tests, test_labels)\n",
    "    ac_scores.append(scores)\n",
    "\n",
    "# changing to misclassification error\n",
    "MCE = [1 - x for x in ac_scores]\n",
    "MCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,20,2), MCE)\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.title('Misclassification Error for different values of k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For K = 3 it is giving the best test accuracy lets check train and test both with other evaluation metrics\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn = knn.fit(X_trains,train_labels)\n",
    "y_train_predict_knn = knn.predict(X_trains)\n",
    "y_test_predict_knn = knn.predict(X_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_knn = knn.score(X_trains,train_labels) \n",
    "acc_test_knn = knn.score(X_tests,test_labels) \n",
    "print('train accuracy :', round(acc_train_knn,4))\n",
    "print('test accuracy :', round(acc_test_knn,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(train_labels, y_train_predict_knn))\n",
    "print('\\n')\n",
    "print(classification_report(train_labels, y_train_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_metrics=classification_report(train_labels, y_train_predict_knn,output_dict=True)\n",
    "df=pd.DataFrame(knn_metrics).transpose()\n",
    "knn_train_precision=round(df.loc[\"1\"][0],2)\n",
    "knn_train_recall=round(df.loc[\"1\"][1],2)\n",
    "knn_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('knn_train_precision ',knn_train_precision)\n",
    "print ('knn_train_recall ',knn_train_recall)\n",
    "print ('knn_train_f1 ',knn_train_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict_knn))\n",
    "print('\\n')\n",
    "print(classification_report(test_labels, y_test_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_metrics=classification_report(test_labels, y_test_predict_knn,output_dict=True)\n",
    "df=pd.DataFrame(knn_metrics).transpose()\n",
    "knn_test_precision=round(df.loc[\"1\"][0],2)\n",
    "knn_test_recall=round(df.loc[\"1\"][1],2)\n",
    "knn_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('knn_test_precision ',knn_test_precision)\n",
    "print ('knn_test_recall ',knn_test_recall)\n",
    "print ('knn_test_f1 ',knn_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Training Data\n",
    "probs_train_knn = knn.predict_proba(X_trains)\n",
    "\n",
    "probs_train_knn = probs_train_knn[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_train_knn = roc_auc_score(train_labels, probs_train_knn)\n",
    "print('AUC for the knn Train Data: %.3f' % auc_train_knn)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_train_knn, tpr_train_knn, thresholds_train_knn = roc_curve(train_labels, probs_train_knn)\n",
    "\n",
    "plt.plot(fpr_train_knn, tpr_train_knn, marker='.',label = 'knn Train',color='red')\n",
    "\n",
    "# Testing Data\n",
    "probs_test_knn = knn.predict_proba(X_tests)\n",
    "\n",
    "probs_test_knn = probs_test_knn[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_test_knn = roc_auc_score(test_labels, probs_test_knn)\n",
    "print('AUC for the knn Test Data: %.3f' % auc_test_knn)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_test_knn, tpr_test_knn, thresholds_test_knn = roc_curve(test_labels, probs_test_knn)\n",
    "\n",
    "plt.plot(fpr_test_knn, tpr_test_knn, marker='.',label = 'knn Test',color='green')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color = 'black')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC Curve for Training & Testing Data for KNN Model ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgcl = BaggingClassifier(base_estimator=cart, random_state=1)\n",
    "param_bgcl = {'n_estimators':list(range(101,502,2)),\n",
    "              'max_samples':list(np.arange(0.05,1,0.01)),\n",
    "              'max_features':list(np.arange(0.01,1,0.005))}\n",
    "grid_search_bgcl = RandomizedSearchCV(estimator = bgcl, param_distributions = param_bgcl, cv = 10,n_jobs=-1,verbose=2)\n",
    "grid_search_bgcl.fit(X_train,train_labels.values.ravel())\n",
    "best_grid_bgcl = grid_search_bgcl.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = best_grid_bgcl.predict(X_train)\n",
    "bg_model_score =best_grid_bgcl.score(X_train, train_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_bgcl = best_grid_bgcl.score(X_train,train_labels) \n",
    "acc_test_bgcl = best_grid_bgcl.score(X_test,test_labels) \n",
    "print('bgcl train accuracy :', round(acc_train_bgcl,4))\n",
    "print('bgcl test accuracy :', round(acc_test_bgcl,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgcl_metrics=classification_report(train_labels, y_train_predict,output_dict=True)\n",
    "df=pd.DataFrame(bgcl_metrics).transpose()\n",
    "bgcl_train_precision=round(df.loc[\"1\"][0],2)\n",
    "bgcl_train_recall=round(df.loc[\"1\"][1],2)\n",
    "bgcl_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('bgcl_train_precision ',bgcl_train_precision)\n",
    "print ('bgcl_train_recall ',bgcl_train_recall)\n",
    "print ('bgcl_train_f1 ',bgcl_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict = best_grid_bgcl.predict(X_test)\n",
    "model_score = best_grid_bgcl.score(X_test, test_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgcl_metrics=classification_report(test_labels, y_test_predict,output_dict=True)\n",
    "df=pd.DataFrame(bgcl_metrics).transpose()\n",
    "bgcl_test_precision=round(df.loc[\"1\"][0],2)\n",
    "bgcl_test_recall=round(df.loc[\"1\"][1],2)\n",
    "bgcl_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('bgcl_test_precision ',bgcl_test_precision)\n",
    "print ('bgcl_test_recall ',bgcl_test_recall)\n",
    "print ('bgcl_test_f1 ',bgcl_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Training Data\n",
    "probs_train_bgcl = best_grid_bgcl.predict_proba(X_train)\n",
    "\n",
    "probs_train_bgcl = probs_train_bgcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_train_bgcl = roc_auc_score(train_labels, probs_train_bgcl)\n",
    "print('AUC for the bgcl Train Data: %.3f' % auc_train_bgcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_train_bgcl, tpr_train_bgcl, thresholds_train_bgcl = roc_curve(train_labels, probs_train_bgcl)\n",
    "\n",
    "plt.plot(fpr_train_bgcl, tpr_train_bgcl, marker='.',label = 'bgcl Train', color='red')\n",
    "\n",
    "# Testing Data\n",
    "probs_test_bgcl = best_grid_bgcl.predict_proba(X_test)\n",
    "\n",
    "probs_test_bgcl = probs_test_bgcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_test_bgcl = roc_auc_score(test_labels, probs_test_bgcl)\n",
    "print('AUC for the bgcl Test Data: %.3f' % auc_test_bgcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_test_bgcl, tpr_test_bgcl, thresholds_test_bgcl = roc_curve(test_labels, probs_test_bgcl)\n",
    "\n",
    "plt.plot(fpr_test_bgcl, tpr_test_bgcl, marker='.',label = 'bgcl Test', color='green')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color = 'black')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC Curve for Training & Testing Data for Bagging Model ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abcl = AdaBoostClassifier(random_state=1)\n",
    "param_abcl = {'n_estimators':list(range(101,502,2))}\n",
    "grid_search_abcl = RandomizedSearchCV(estimator = abcl, param_distributions = param_abcl, cv = 10,n_jobs=-1,verbose=2)\n",
    "grid_search_abcl.fit(X_train,train_labels.values.ravel())\n",
    "best_grid_abcl = grid_search_abcl.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_abcl = best_grid_abcl.score(X_train,train_labels) \n",
    "acc_test_abcl = best_grid_abcl.score(X_test,test_labels) \n",
    "print('abcl train accuracy :', round(acc_train_abcl,4))\n",
    "print('abcl test accuracy :', round(acc_test_abcl,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = best_grid_abcl.predict(X_train)\n",
    "model_score = best_grid_abcl.score(X_train, train_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, y_train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcl_metrics=classification_report(train_labels, y_train_predict,output_dict=True)\n",
    "df=pd.DataFrame(abcl_metrics).transpose()\n",
    "abcl_train_precision=round(df.loc[\"1\"][0],2)\n",
    "abcl_train_recall=round(df.loc[\"1\"][1],2)\n",
    "abcl_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('abcl_train_precision ',abcl_train_precision)\n",
    "print ('abcl_train_recall ',abcl_train_recall)\n",
    "print ('abcl_train_f1 ',abcl_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_bgcl = best_grid_abcl.predict(X_test)\n",
    "model_score = best_grid_abcl.score(X_test, test_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abcl_metrics=classification_report(test_labels, y_test_predict,output_dict=True)\n",
    "df=pd.DataFrame(abcl_metrics).transpose()\n",
    "abcl_test_precision=round(df.loc[\"1\"][0],2)\n",
    "abcl_test_recall=round(df.loc[\"1\"][1],2)\n",
    "abcl_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('abcl_test_precision ',abcl_test_precision)\n",
    "print ('abcl_test_recall ',abcl_test_recall)\n",
    "print ('abcl_test_f1 ',abcl_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Training Data\n",
    "probs_train_abcl = best_grid_abcl.predict_proba(X_train)\n",
    "\n",
    "probs_train_abcl = probs_train_abcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_train_abcl = roc_auc_score(train_labels, probs_train_abcl)\n",
    "print('AUC for the abcl Train Data: %.3f' % auc_train_abcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_train_abcl, tpr_train_abcl, thresholds_train_abcl = roc_curve(train_labels, probs_train_abcl)\n",
    "\n",
    "plt.plot(fpr_train_abcl, tpr_train_abcl, marker='.',label = 'abcl Train',color='red')\n",
    "\n",
    "# Testing Data\n",
    "probs_test_abcl = best_grid_abcl.predict_proba(X_test)\n",
    "\n",
    "probs_test_abcl = probs_test_abcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_test_abcl = roc_auc_score(test_labels, probs_test_abcl)\n",
    "print('AUC for the abcl Test Data: %.3f' % auc_test_abcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_test_abcl, tpr_test_abcl, thresholds_test_abcl = roc_curve(test_labels, probs_test_abcl)\n",
    "\n",
    "plt.plot(fpr_test_abcl, tpr_test_abcl, marker='.',label = 'abcl Test',color='green')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color = 'black')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC Curve for Training & Testing Data for Ada Boosting Model ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcl = GradientBoostingClassifier(random_state=1)\n",
    "param_gbcl = {'learning_rate':list(np.arange(0.01,0.019,0.0000005)),\n",
    "             'n_estimators':list(np.arange(101,121,2)),\n",
    "             'max_depth':list(np.arange(5,11))}\n",
    "grid_search_gbcl = RandomizedSearchCV(estimator = gbcl, param_distributions = param_gbcl, cv = 10,n_jobs=-1,scoring='precision',verbose=2,random_state=1)\n",
    "grid_search_gbcl.fit(X_train,train_labels.values.ravel())\n",
    "best_grid_gbcl=grid_search_gbcl.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_gbcl = best_grid_gbcl.score(X_train,train_labels) \n",
    "acc_test_gbcl = best_grid_gbcl.score(X_test,test_labels) \n",
    "print('gbcl train accuracy :', round(acc_train_gbcl,4))\n",
    "print('gbcl test accuracy :', round(acc_test_gbcl,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict_gbcl = best_grid_gbcl.predict(X_train)\n",
    "model_score = best_grid_gbcl.score(X_train, train_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(train_labels, y_train_predict_gbcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(train_labels, y_train_predict_gbcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcl_metrics=classification_report(train_labels, y_train_predict_gbcl,output_dict=True)\n",
    "df=pd.DataFrame(gbcl_metrics).transpose()\n",
    "gbcl_train_precision=round(df.loc[\"1\"][0],2)\n",
    "gbcl_train_recall=round(df.loc[\"1\"][1],2)\n",
    "gbcl_train_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('gbcl_train_precision ',gbcl_train_precision)\n",
    "print ('gbcl_train_recall ',gbcl_train_recall)\n",
    "print ('gbcl_train_f1 ',gbcl_train_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict_gbcl = best_grid_gbcl.predict(X_test)\n",
    "model_score = best_grid_gbcl.score(X_test, test_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict_gbcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_predict_gbcl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbcl_metrics=classification_report(test_labels, y_test_predict_gbcl,output_dict=True)\n",
    "df=pd.DataFrame(gbcl_metrics).transpose()\n",
    "gbcl_test_precision=round(df.loc[\"1\"][0],2)\n",
    "gbcl_test_recall=round(df.loc[\"1\"][1],2)\n",
    "gbcl_test_f1=round(df.loc[\"1\"][2],2)\n",
    "print ('gbcl_test_precision ',gbcl_test_precision)\n",
    "print ('gbcl_test_recall ',gbcl_test_recall)\n",
    "print ('gbcl_test_f1 ',gbcl_test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "# Training Data\n",
    "probs_train_gbcl = best_grid_gbcl.predict_proba(X_train)\n",
    "\n",
    "probs_train_gbcl = probs_train_gbcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_train_gbcl = roc_auc_score(train_labels, probs_train_gbcl)\n",
    "print('AUC for the gbcl Train Data: %.3f' % auc_train_gbcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_train_gbcl, tpr_train_gbcl, thresholds_train_gbcl = roc_curve(train_labels, probs_train_gbcl)\n",
    "\n",
    "plt.plot(fpr_train_gbcl, tpr_train_gbcl, marker='.',label = 'gbcl Train',color='red')\n",
    "\n",
    "# Testing Data\n",
    "probs_test_gbcl = best_grid_gbcl.predict_proba(X_test)\n",
    "\n",
    "probs_test_gbcl = probs_test_gbcl[:, 1]\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "auc_test_gbcl = roc_auc_score(test_labels, probs_test_gbcl)\n",
    "print('AUC for the gbcl Test Data: %.3f' % auc_test_gbcl)\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr_test_gbcl, tpr_test_gbcl, thresholds_test_gbcl = roc_curve(test_labels, probs_test_gbcl)\n",
    "\n",
    "plt.plot(fpr_test_gbcl, tpr_test_gbcl, marker='.',label = 'gbcl Test',color='green')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color = 'black')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC Curve for Training & Testing Data for Gradient Boosting Model ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smote data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, train_labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN With SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "KNN_SM_model=KNeighborsClassifier()\n",
    "KNN_SM_model.fit(X_train_res,y_train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Matrix on train data set\n",
    "y_train_predict_knn = KNN_SM_model.predict(X_train_res)\n",
    "model_score = KNN_SM_model.score(X_train_res, y_train_res)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train_res, y_train_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train_res, y_train_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Matrix on test data set\n",
    "y_test_predict_knn = KNN_SM_model.predict(X_test)\n",
    "model_score = KNN_SM_model.score(X_test, test_labels)\n",
    "print(model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, y_test_predict_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, y_test_predict_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the performance metrics from the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=['Accuracy', 'AUC', 'Recall','Precision','F1 Score']\n",
    "data = pd.DataFrame({'CART Train':[cart_train_acc,cart_train_auc,cart_train_recall,cart_train_precision,cart_train_f1],\n",
    "        'CART Test':[cart_test_acc,cart_test_auc,cart_test_recall,cart_test_precision,cart_test_f1],\n",
    "        'Random Forest Train':[rf_train_acc,rf_train_auc,rf_train_recall,rf_train_precision,rf_train_f1],\n",
    "        'Random Forest Test':[rf_test_acc,rf_test_auc,rf_test_recall,rf_test_precision,rf_test_f1],\n",
    "        'Logistic Regression Train':[lr_train_acc,lr_train_auc,lr_train_recall,lr_train_precision,lr_train_f1],\n",
    "        'Logistic Regression Test':[lr_test_acc,lr_test_auc,lr_test_recall,lr_test_precision,lr_test_f1],\n",
    "        'Neural Network Train':[nn_train_acc,nn_train_auc,nn_train_recall,nn_train_precision,nn_train_f1],\n",
    "        'Neural Network Test':[nn_test_acc,nn_test_auc,nn_test_recall,nn_test_precision,nn_test_f1],\n",
    "        'KNN Train':[acc_train_knn,auc_train_knn,knn_train_recall,knn_train_precision,knn_train_f1],\n",
    "        'KNN Test':[acc_test_knn,auc_test_knn,knn_test_recall,knn_test_precision,knn_test_f1],\n",
    "        'BGCL Train':[acc_train_bgcl, auc_train_bgcl, bgcl_train_precision, bgcl_train_recall, bgcl_train_f1],\n",
    "        'BGCL Test':[acc_test_bgcl, auc_test_bgcl,bgcl_test_precision, bgcl_test_recall, bgcl_test_f1],\n",
    "        'GBCL Train':[acc_train_bgcl, auc_train_gbcl, gbcl_train_precision, gbcl_train_recall, gbcl_train_f1],\n",
    "        'GBCL Test':[acc_test_gbcl, auc_test_gbcl,gbcl_test_precision, gbcl_test_recall, gbcl_test_f1],\n",
    "        'ABCL Train':[acc_train_abcl, auc_train_abcl, abcl_train_precision, abcl_train_recall, abcl_train_f1],\n",
    "        'ABCL Test':[acc_test_abcl, auc_test_abcl,abcl_test_precision, abcl_test_recall, abcl_test_f1]},index=index)\n",
    "round(data,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve for all the models on the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_train_fpr, cart_train_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_train_fpr,rf_train_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_train_fpr,nn_train_tpr,color='black',label=\"NN\")\n",
    "plt.plot(lr_train_fpr,lr_train_tpr,color='blue',label=\"LR\")\n",
    "plt.plot(fpr_train_knn,tpr_train_knn,color='orange',label=\"KNN\")\n",
    "plt.plot(fpr_train_bgcl, tpr_train_bgcl,color='purple',label=\"BGCL\")\n",
    "plt.plot(fpr_train_gbcl, tpr_train_gbcl,color='pink',label=\"GBCL\")\n",
    "plt.plot(fpr_train_abcl, tpr_train_abcl,color='violet',label=\"ABCL\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve for all the models on the Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(cart_test_fpr, cart_test_tpr,color='red',label=\"CART\")\n",
    "plt.plot(rf_test_fpr,rf_test_tpr,color='green',label=\"RF\")\n",
    "plt.plot(nn_test_fpr,nn_test_tpr,color='black',label=\"NN\")\n",
    "plt.plot(lr_test_fpr,lr_test_tpr,color='blue',label=\"LR\")\n",
    "plt.plot(fpr_test_knn,tpr_test_knn,color='orange',label=\"KNN\")\n",
    "plt.plot(fpr_test_bgcl, tpr_test_bgcl,color='purple',label=\"BGCL\")\n",
    "plt.plot(fpr_test_gbcl, tpr_test_gbcl,color='pink',label=\"GBCL\")\n",
    "plt.plot(fpr_test_abcl, tpr_test_abcl,color='violet',label=\"ABCL\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='purple'>THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
